{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Tennis\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import config\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ddpg import DDPGAgent\n",
    "# from config import Config\n",
    "from collections import deque\n",
    "from unityagents import UnityEnvironment\n",
    "from utilities import ReplayBuffer\n",
    "# from agent.maddpg import MADDPG, MultiAgentConfig\n",
    "# from IPython.display import clear_output\n",
    "# from unityagents import UnityEnvironment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"/home/maurice/Documents/udacity_new/data/Tennis_Linux_NoVis/Tennis.x86_64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default environment parameters\n",
    "brain_name  = env.brain_names[0]\n",
    "brain       = env.brains[brain_name]\n",
    "env_info    = env.reset(train_mode=True)[brain_name]\n",
    "num_agents  = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "state_size  = env_info.vector_observations.shape[1]\n",
    "\n",
    "# state = env_info.vector_observations\n",
    "\n",
    "# print(action_size, state_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_to_tensor(input_list):\n",
    "    make_tensor = lambda x: torch.tensor(x, dtype=torch.float)\n",
    "    return list(map(make_tensor, zip(*input_list)))\n",
    "\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPG():\n",
    "    def __init__(self, state_size, action_size, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.seed   = config.seed \n",
    "        self.agents = [DDPGAgent(state_size, action_size, self.config) for _ in range(num_agents)]\n",
    "        self.iter   = 0 \n",
    "        self.learn_iter = 0\n",
    "        \n",
    "        self.beta_function = lambda x: min(1.0, self.config.beta + x * (1.0 - self.config.beta) / self.config.beta_decay)\n",
    "        self.memory = ReplayBuffer(self.config.buffer_size, self.config.seed)\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        for ddpg_agent in self.agents:\n",
    "            ddpg_agent.reset()\n",
    "    \n",
    "    def get_actors(self):\n",
    "        \"\"\"get actors of all the agents in the MADDPG object\"\"\"\n",
    "        actors = [ddpg_agent.actor for ddpg_agent in self.agents]\n",
    "        return actors\n",
    "    \n",
    "    \n",
    "    def get_target_actors(self):\n",
    "        \"\"\"get target_actors of all the agents in the MADDPG object\"\"\"\n",
    "        target_actors = [ddpg_agent.target_actor for ddpg_agent in self.agents]\n",
    "        return target_actors\n",
    "    \n",
    "    \n",
    "    def act(self, obs_all_agents, noise=0.0):\n",
    "        \"\"\"get actions from all agents in the MADDPG object\"\"\"\n",
    "        obs_all_agents = torch.tensor(obs_all_agents, dtype=torch.float).to(self.config.device)\n",
    "        actions = [np.clip(agent.act(obs, noise).cpu().data.numpy(), -1, 1) \n",
    "                   for agent, obs in zip(self.agents, obs_all_agents)]\n",
    "        return actions\n",
    "    \n",
    "    \n",
    "    def target_act(self, obs_all_agents, noise=0.0):\n",
    "        \"\"\"get target network actions from all the agents in the MADDPG object \"\"\"\n",
    "        target_actions = [ddpg_agent.target_act(obs, noise) for ddpg_agent, obs \n",
    "                          in zip(self.agents, obs_all_agents)]\n",
    "        return target_actions\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "        self.iter += 1\n",
    "\n",
    "        if(len(self.memory) >= self.config.batch_size) and self.iter % self.config.update_every == 0:\n",
    "            beta = self.beta_function(self.learn_iter)\n",
    "            for i in range(len(self.agents)):\n",
    "                samples = self.memory.sample(self.config.batch_size, beta)\n",
    "                self.update(samples, i)\n",
    "            self.learn_iter += 1\n",
    "            self.update_targets()\n",
    "\n",
    "    def _prep_samples(self, samples):\n",
    "        convert = lambda x: torch.tensor(x, dtype=torch.float).to(self.config.device)\n",
    "\n",
    "        obs, action, reward, next_obs, done, weights, idx = samples\n",
    "\n",
    "        obs           = np.rollaxis(obs, 1)\n",
    "        next_obs      = np.rollaxis(next_obs, 1)\n",
    "        obs_full      = np.hstack(obs)\n",
    "        next_obs_full = np.hstack(next_obs)\n",
    "\n",
    "        obs           = convert(obs)\n",
    "        obs_full      = convert(obs_full)\n",
    "        action        = convert(action)\n",
    "        reward        = convert(reward)\n",
    "        next_obs      = convert(next_obs)\n",
    "        next_obs_full = convert(next_obs_full)\n",
    "        done          = convert(np.float32(done))\n",
    "        weights       = convert(weights)\n",
    "\n",
    "        return obs, obs_full, action, reward, next_obs, next_obs_full, done, idx, weights            \n",
    "    \n",
    "    def update(self, samples, agent_number):\n",
    "        \"\"\"update the critics and actors of all the agents \"\"\"\n",
    "\n",
    "        # need to transpose each element of the samples\n",
    "        # to flip obs[parallel_agent][agent_number] to\n",
    "        # obs[agent_number][parallel_agent]\n",
    "        \n",
    "        \n",
    "        obs, obs_full, action, reward, next_obs, next_obs_full, done, idx, weights = self._prep_samples(samples)        \n",
    "#         obs, obs_full, action, reward, next_obs, next_obs_full, done = map(transpose_to_tensor, samples)\n",
    "\n",
    "#         obs_full = torch.stack(obs_full)\n",
    "#         next_obs_full = torch.stack(next_obs_full)\n",
    "        \n",
    "        agent = self.agents[agent_number]\n",
    "        agent.critic_optimizer.zero_grad()\n",
    "\n",
    "        #critic loss = batch mean of (y- Q(s,a) from target network)^2\n",
    "        #y = reward of this timestep + discount * Q(st+1,at+1) from target network\n",
    "        target_actions = self.target_act(next_obs)\n",
    "        target_actions = torch.cat(target_actions, dim=1).detach()\n",
    "        \n",
    "        target_critic_input = torch.cat((next_obs_full,target_actions), dim=1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_next = agent.target_critic(target_critic_input)\n",
    "        \n",
    "        \n",
    "#         print('\\ncheck A = ', reward[agent_number].view(-1, 1))\n",
    "#         print('check B = ', self.config.gamma)\n",
    "#         print('check C = ', q_next)\n",
    "#         print('check D = ', 1 - done[agent_number].view(-1, 1))\n",
    "        \n",
    "#         y = reward[agent_number].view(-1, 1) + self.config.gamma * q_next * (1 - done[agent_number].view(-1, 1))\n",
    "        y = reward[..., agent_number].unsqueeze(1) + self.config.gamma * q_next * (1 - done[..., agent_number].unsqueeze(1))\n",
    "        \n",
    "#         action = torch.cat(action, dim=1)?\n",
    "        critic_input = torch.cat((obs_full, action.view(self.config.batch_size, -1)), dim=1)\n",
    "        q = agent.critic(critic_input)\n",
    "\n",
    "        huber_loss = torch.nn.SmoothL1Loss()\n",
    "        critic_loss = huber_loss(q, y.detach())\n",
    "        critic_loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(agent.critic.parameters(), 0.5)\n",
    "        agent.critic_optimizer.step()\n",
    "\n",
    "        #update actor network using policy gradient\n",
    "        agent.actor_optimizer.zero_grad()\n",
    "        # make input to agent\n",
    "        # detach the other agents to save computation\n",
    "        # saves some time for computing derivative\n",
    "        q_input = [ self.agents[i].actor(ob) if i == agent_number \\\n",
    "                   else self.agents[i].actor(ob).detach()\n",
    "                   for i, ob in enumerate(obs) ]\n",
    "                \n",
    "        q_input = torch.cat(q_input, dim=1)\n",
    "        # combine all the actions and observations for input to critic\n",
    "        # many of the obs are redundant, and obs[1] contains all useful information already\n",
    "        q_input2 = torch.cat((obs_full, q_input), dim=1)\n",
    "        \n",
    "        # get the policy gradient\n",
    "        actor_loss = -agent.critic(q_input2).mean()\n",
    "        actor_loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(agent.actor.parameters(),0.5)\n",
    "        agent.actor_optimizer.step()\n",
    "\n",
    "        al = actor_loss.cpu().detach().item()\n",
    "        cl = critic_loss.cpu().detach().item()\n",
    "#         logger.add_scalars('agent%i/losses' % agent_number,\n",
    "#                            {'critic loss': cl,\n",
    "#                             'actor_loss': al},\n",
    "#                            self.iter)\n",
    "\n",
    "    def update_targets(self):\n",
    "        \"\"\"soft update targets\"\"\"\n",
    "        self.iter += 1\n",
    "        for ddpg_agent in self.agents:\n",
    "            soft_update(ddpg_agent.target_actor, ddpg_agent.actor, self.config.tau)\n",
    "            soft_update(ddpg_agent.target_critic, ddpg_agent.critic, self.config.tau)\n",
    "      \n",
    "    def save_checkpoints(self, file_head):\n",
    "        for i in range(len(self.agents)):\n",
    "            file_name = file_head + 'agent{}_'.format(i)\n",
    "            torch.save(self.agents[i].actor.state_dict(), file_name + '_actor.pth')\n",
    "            torch.save(self.agents[i].critic.state_dict(), file_name + '_critic.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(maddpg, n_eps=10000, max_steps=1000):\n",
    "    \n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    train_mode = True\n",
    "    \n",
    "    frame_num = 0\n",
    "    current_max = 1.0\n",
    "    \n",
    "    # training loop\n",
    "    for i_eps in range(n_eps):\n",
    "        env_info = env.reset(train_mode=train_mode)[brain_name]\n",
    "        state    = env_info.vector_observations\n",
    "        maddpg.reset()\n",
    "        agent_scores = np.zeros(num_agents)\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action     = maddpg.act(state, 1.0)\n",
    "            env_info   = env.step(action)[brain_name] \n",
    "            reward     = env_info.rewards\n",
    "            next_state = env_info.vector_observations\n",
    "            done       = env_info.local_done\n",
    "            maddpg.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            state      = next_state\n",
    "            agent_scores += reward\n",
    "            frame_num += 1\n",
    "            \n",
    "            if np.any(done): break\n",
    "                \n",
    "        max_score = np.max(agent_scores)\n",
    "        scores_window.append(max_score)\n",
    "        scores.append(max_score)                \n",
    "    \n",
    "        # change all these print statements    \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_eps, np.mean(scores_window)), end=\"\")\n",
    "        \n",
    "        if i_eps % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_eps, np.mean(scores_window)))\n",
    "\n",
    "        if max_score > current_max:\n",
    "            maddpg.save_checkpoints('checkpoints/checkpoint_max_')\n",
    "            current_max = max_score\n",
    "            \n",
    "        if np.mean(scores_window) >= 0.5:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_eps, np.mean(scores_window)))\n",
    "            break\n",
    "            \n",
    "            \n",
    "    return scores\n",
    "                \n",
    "    \n",
    "    \n",
    "def plot_scores(scores, title=\"\"):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.title(title)\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tAverage Score: 0.00\n",
      "Episode 100\tAverage Score: 0.01\n",
      "Episode 200\tAverage Score: 0.00\n",
      "Episode 300\tAverage Score: 0.00\n",
      "Episode 400\tAverage Score: 0.00\n",
      "Episode 500\tAverage Score: 0.01\n",
      "Episode 600\tAverage Score: 0.01\n",
      "Episode 700\tAverage Score: 0.02\n",
      "Episode 800\tAverage Score: 0.01\n",
      "Episode 900\tAverage Score: 0.01\n",
      "Episode 1000\tAverage Score: 0.02\n",
      "Episode 1100\tAverage Score: 0.05\n",
      "Episode 1200\tAverage Score: 0.04\n",
      "Episode 1300\tAverage Score: 0.05\n",
      "Episode 1400\tAverage Score: 0.04\n",
      "Episode 1500\tAverage Score: 0.06\n",
      "Episode 1600\tAverage Score: 0.08\n",
      "Episode 1700\tAverage Score: 0.08\n",
      "Episode 1800\tAverage Score: 0.11\n",
      "Episode 1900\tAverage Score: 0.15\n",
      "Episode 2000\tAverage Score: 0.14\n",
      "Episode 2100\tAverage Score: 0.15\n",
      "Episode 2200\tAverage Score: 0.18\n",
      "Episode 2300\tAverage Score: 0.18\n",
      "Episode 2400\tAverage Score: 0.20\n",
      "Episode 2500\tAverage Score: 0.25\n",
      "Episode 2511\tAverage Score: 0.25"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'checkpoints/checkpoint_max_agent0__actor.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fc2fc3a2ba8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mmaddpg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMADDPG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# print(config)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaddpg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mplot_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a8afa3ecdd59>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(maddpg, n_eps, max_steps)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_score\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mcurrent_max\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mmaddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoints/checkpoint_max_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mcurrent_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-4d55e9c6c2a7>\u001b[0m in \u001b[0;36msave_checkpoints\u001b[0;34m(self, file_head)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_head\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'agent{}_'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_actor.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_critic.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \"\"\"\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoints/checkpoint_max_agent0__actor.pth'"
     ]
    }
   ],
   "source": [
    "# config = config()\n",
    "config.seed = 0\n",
    "config.buffer_size = int(1e5)\n",
    "config.batch_size = 512\n",
    "config.gamma = 0.99\n",
    "config.tau = 2e-1\n",
    "\n",
    "config.actor_hidden_sizes = [256, 128]\n",
    "config.lr_actor = 1e-4\n",
    "config.critic_hidden_sizes = [256, 128]\n",
    "config.lr_critic = 3e-4\n",
    "config.critic_weight_decay = 0.0\n",
    "\n",
    "config.mu = 0.\n",
    "config.theta = 0.15\n",
    "config.sigma = 0.2\n",
    "\n",
    "config.update_every = 5\n",
    "config.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# config.prioritized_replay = True\n",
    "config.beta = 0.4\n",
    "config.beta_decay = 10000\n",
    "# config.alpha = 0.6\n",
    "\n",
    "# print(config.batch_size)\n",
    "\n",
    "maddpg = MADDPG(state_size, action_size, config)\n",
    "# print(config)\n",
    "scores = train(maddpg, 3000, 2000)\n",
    "\n",
    "plot_scores(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

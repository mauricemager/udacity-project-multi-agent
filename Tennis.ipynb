{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Tennis\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import config\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data.maddpg import MADDPG\n",
    "from collections import deque\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialize the unity Tennis environment\n",
    "\n",
    "make sure the path to file_name is corrent and matching to your system. \n",
    "In here you should locate the Tennis environment file on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"/home/maurice/Documents/udacity_new/data/Tennis_Linux_NoVis/Tennis.x86_64\")\n",
    "# env = UnityEnvironment(file_name=\"/home/maurice/Documents/udacity_new/data/Tennis_Linux/Tennis.x86_64\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get default environment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_name  = env.brain_names[0]\n",
    "brain       = env.brains[brain_name]\n",
    "env_info    = env.reset(train_mode=True)[brain_name]\n",
    "num_agents  = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "state_size  = env_info.vector_observations.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Set-up training algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(maddpg, n_eps=10000, max_steps=2000):\n",
    "    \n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    train_mode = True\n",
    "    \n",
    "#     frame_num = 0\n",
    "    current_max = 1.0\n",
    "\n",
    "    # training loop\n",
    "    for i_eps in range(n_eps):\n",
    "        env_info = env.reset(train_mode=train_mode)[brain_name]\n",
    "        state    = env_info.vector_observations\n",
    "        maddpg.reset()\n",
    "        agent_scores = np.zeros(num_agents)\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action     = maddpg.act(state, 1.0)\n",
    "            env_info   = env.step(action)[brain_name] \n",
    "            reward     = env_info.rewards\n",
    "            next_state = env_info.vector_observations\n",
    "            done       = env_info.local_done\n",
    "            maddpg.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            state      = next_state\n",
    "            agent_scores += reward\n",
    "#             frame_num += 1\n",
    "            \n",
    "            if np.any(done): break\n",
    "                \n",
    "        max_score = np.max(agent_scores)\n",
    "        scores_window.append(max_score)\n",
    "        scores.append(max_score)                \n",
    "    \n",
    "        print('\\rTraining Episode: {}\\tAverage Score: {:.2f}'.format(i_eps, np.mean(scores_window)), end=\"\")\n",
    "        \n",
    "        if i_eps % 100 == 0:\n",
    "            print('\\rTraining Episode: {}\\tAverage Score: {:.2f}'.format(i_eps, np.mean(scores_window)))\n",
    "\n",
    "        if max_score > current_max:\n",
    "            current_max = max_score\n",
    "            for i in range(len(maddpg.agents)):\n",
    "                torch.save(maddpg.agents[i].actor.state_dict(),  'checkpoints/actor'  + str(i) +'checkpoint.pth')\n",
    "                torch.save(maddpg.agents[i].critic.state_dict(), 'checkpoints/critic' + str(i) +'checkpoint.pth')\n",
    "\n",
    "                \n",
    "        if np.mean(scores_window) >= 0.5:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_eps, np.mean(scores_window)))\n",
    "            break\n",
    "            \n",
    "            \n",
    "    return scores\n",
    "                \n",
    "    \n",
    "    \n",
    "def plot_scores(scores, title=\"\"):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.title(title)\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Configure hyperparameters and perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup config datastruct with hyperparameters\n",
    "config.seed        = 1\n",
    "config.buffer_size = int(1e5)\n",
    "config.batch_size  = 512\n",
    "config.gamma       = 0.99\n",
    "config.tau         = 2e-1\n",
    "config.mu          = 0.0\n",
    "config.theta       = 0.15\n",
    "config.sigma       = 0.2\n",
    "config.beta        = 0.4\n",
    "config.beta_decay  = 10000\n",
    "\n",
    "config.action_size         = action_size\n",
    "config.state_size          = state_size\n",
    "config.num_agents          = num_agents\n",
    "config.actor_hidden_sizes  = [128, 64]\n",
    "config.critic_hidden_sizes = [128, 64]\n",
    "config.lr_actor            = 1e-4\n",
    "config.lr_critic           = 3e-4\n",
    "config.critic_weight_decay = 0.0\n",
    "config.update_every        = 5\n",
    "\n",
    "config.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run training algorithm and plot results\n",
    "maddpg = MADDPG(config)\n",
    "tic    = time.perf_counter()\n",
    "scores = train(maddpg, 5000, 2000)\n",
    "toc    = time.perf_counter()\n",
    "\n",
    "plot_scores(scores)\n",
    "print('training took ' + str(datetime.timedelta(seconds=round(toc - tic))) + ' [h:mm:ss]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Watch a smart agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the no_vis environment and open visual environment to see results\n",
    "env.close()\n",
    "env = UnityEnvironment(file_name=\"/home/maurice/Documents/udacity_new/data/Tennis_Linux/Tennis.x86_64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Max score = 0.10000000149011612\n",
      " Max score = 0.30000000447034836\n",
      " Max score = 2.400000035762787\n"
     ]
    }
   ],
   "source": [
    "def run_smart_agent(trained_maddpg):\n",
    "\n",
    "    for i in range(3): \n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        state    = env_info.vector_observations\n",
    "        trained_maddpg.reset()\n",
    "        agent_scores = np.zeros(num_agents)\n",
    "        \n",
    "        while True:\n",
    "            action     = trained_maddpg.act(state, 0.0)\n",
    "            env_info   = env.step(action)[brain_name] \n",
    "            reward     = env_info.rewards\n",
    "            next_state = env_info.vector_observations\n",
    "            done       = env_info.local_done  \n",
    "            state      = next_state\n",
    "            agent_scores += reward\n",
    "            \n",
    "            if np.any(done): break\n",
    "        print(f' Max score = {np.max(agent_scores)}')\n",
    "        \n",
    "                \n",
    "trained_maddpg = MADDPG(config)\n",
    "trained_maddpg.load_checkpoints()\n",
    "run_smart_agent(trained_maddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Tennis\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ddpg import DDPGAgent\n",
    "from config import Config\n",
    "from collections import deque\n",
    "from unityagents import UnityEnvironment\n",
    "from utilities import ReplayBuffer\n",
    "# from agent.maddpg import MADDPG, MultiAgentConfig\n",
    "# from IPython.display import clear_output\n",
    "# from unityagents import UnityEnvironment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"/home/maurice/Documents/udacity_new/data/Tennis_Linux_NoVis/Tennis.x86_64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default environment parameters\n",
    "brain_name  = env.brain_names[0]\n",
    "brain       = env.brains[brain_name]\n",
    "env_info    = env.reset(train_mode=True)[brain_name]\n",
    "num_agents  = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "state_size  = env_info.vector_observations.shape[1]\n",
    "\n",
    "# state = env_info.vector_observations\n",
    "\n",
    "# print(action_size, state_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPG():\n",
    "    def __init__(self, state_size, action_size, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.seed   = config.seed \n",
    "        self.agents = [DDPGAgent(state_size, action_size, self.config) for _ in range(num_agents)]\n",
    "        self.iter   = 0 \n",
    "        \n",
    "        self.beta_function = lambda x: min(1.0, self.config.beta + x * (1.0 - self.config.beta) / self.config.beta_decay)\n",
    "        self.memory = ReplayBuffer(self.config.buffer_size, self.config.seed)\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        for ddpg_agent in self.agents:\n",
    "            ddpg_agent.reset()\n",
    "    \n",
    "    def get_actors(self):\n",
    "        \"\"\"get actors of all the agents in the MADDPG object\"\"\"\n",
    "        actors = [ddpg_agent.actor for ddpg_agent in self.agents]\n",
    "        return actors\n",
    "    \n",
    "    \n",
    "    def get_target_actors(self):\n",
    "        \"\"\"get target_actors of all the agents in the MADDPG object\"\"\"\n",
    "        target_actors = [ddpg_agent.target_actor for ddpg_agent in self.agents]\n",
    "        return target_actors\n",
    "    \n",
    "    \n",
    "    def act(self, obs_all_agents, noise=0.0):\n",
    "        \"\"\"get actions from all agents in the MADDPG object\"\"\"\n",
    "        obs_all_agents = torch.tensor(obs_all_agents, dtype=torch.float).to(self.config.device)\n",
    "        actions = [np.clip(agent.act(obs, noise).cpu().data.numpy(), -1, 1) \n",
    "                   for agent, obs in zip(self.maddpg_agent, obs_all_agents)]\n",
    "        return actions\n",
    "    \n",
    "    \n",
    "    def target_act(self, obs_all_agents, noise=0.0):\n",
    "        \"\"\"get target network actions from all the agents in the MADDPG object \"\"\"\n",
    "        target_actions = [ddpg_agent.target_act(obs, noise) for ddpg_agent, obs \n",
    "                          in zip(self.agents, obs_all_agents)]\n",
    "        return target_actions\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "        self.iter += 1\n",
    "\n",
    "        if(len(self.memory) >= self.config.batch_size) and self.iter % self.config.update_every == 0:\n",
    "            beta = self.beta_function(self.learn_iter)\n",
    "            for i in range(len(self.maddpg_agent)):\n",
    "                samples = self.memory.sample(self.config.batch_size, beta)\n",
    "                self.update_tuned(samples, i)\n",
    "            self.learn_iter += 1\n",
    "            self.update_targets()\n",
    "    \n",
    "    def update(self, samples, agent_number, logger):\n",
    "        \"\"\"update the critics and actors of all the agents \"\"\"\n",
    "\n",
    "        # need to transpose each element of the samples\n",
    "        # to flip obs[parallel_agent][agent_number] to\n",
    "        # obs[agent_number][parallel_agent]\n",
    "        obs, obs_full, action, reward, next_obs, next_obs_full, done = map(transpose_to_tensor, samples)\n",
    "\n",
    "        obs_full = torch.stack(obs_full)\n",
    "        next_obs_full = torch.stack(next_obs_full)\n",
    "        \n",
    "        agent = self.maddpg_agent[agent_number]\n",
    "        agent.critic_optimizer.zero_grad()\n",
    "\n",
    "        #critic loss = batch mean of (y- Q(s,a) from target network)^2\n",
    "        #y = reward of this timestep + discount * Q(st+1,at+1) from target network\n",
    "        target_actions = self.target_act(next_obs)\n",
    "        target_actions = torch.cat(target_actions, dim=1)\n",
    "        \n",
    "        target_critic_input = torch.cat((next_obs_full.t(),target_actions), dim=1).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_next = agent.target_critic(target_critic_input)\n",
    "        \n",
    "        y = reward[agent_number].view(-1, 1) + self.discount_factor * q_next * (1 - done[agent_number].view(-1, 1))\n",
    "        action = torch.cat(action, dim=1)\n",
    "        critic_input = torch.cat((obs_full.t(), action), dim=1).to(device)\n",
    "        q = agent.critic(critic_input)\n",
    "\n",
    "        huber_loss = torch.nn.SmoothL1Loss()\n",
    "        critic_loss = huber_loss(q, y.detach())\n",
    "        critic_loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(agent.critic.parameters(), 0.5)\n",
    "        agent.critic_optimizer.step()\n",
    "\n",
    "        #update actor network using policy gradient\n",
    "        agent.actor_optimizer.zero_grad()\n",
    "        # make input to agent\n",
    "        # detach the other agents to save computation\n",
    "        # saves some time for computing derivative\n",
    "        q_input = [ self.maddpg_agent[i].actor(ob) if i == agent_number \\\n",
    "                   else self.maddpg_agent[i].actor(ob).detach()\n",
    "                   for i, ob in enumerate(obs) ]\n",
    "                \n",
    "        q_input = torch.cat(q_input, dim=1)\n",
    "        # combine all the actions and observations for input to critic\n",
    "        # many of the obs are redundant, and obs[1] contains all useful information already\n",
    "        q_input2 = torch.cat((obs_full.t(), q_input), dim=1)\n",
    "        \n",
    "        # get the policy gradient\n",
    "        actor_loss = -agent.critic(q_input2).mean()\n",
    "        actor_loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(agent.actor.parameters(),0.5)\n",
    "        agent.actor_optimizer.step()\n",
    "\n",
    "        al = actor_loss.cpu().detach().item()\n",
    "        cl = critic_loss.cpu().detach().item()\n",
    "        logger.add_scalars('agent%i/losses' % agent_number,\n",
    "                           {'critic loss': cl,\n",
    "                            'actor_loss': al},\n",
    "                           self.iter)\n",
    "\n",
    "    def update_targets(self):\n",
    "        \"\"\"soft update targets\"\"\"\n",
    "        self.iter += 1\n",
    "        for ddpg_agent in self.maddpg_agent:\n",
    "            soft_update(ddpg_agent.target_actor, ddpg_agent.actor, self.tau)\n",
    "            soft_update(ddpg_agent.target_critic, ddpg_agent.critic, self.tau)\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(maddpg, n_eps=10000, max_steps=1000):\n",
    "    \n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    train_mode = True\n",
    "    \n",
    "    frame_num = 0\n",
    "    current_max = 1.0\n",
    "    \n",
    "    # training loop\n",
    "    for i_eps in range(n_eps):\n",
    "        env_info = env.reset(train_mode=train_mode)[brain_name]\n",
    "        state    = env_info.vector_observations\n",
    "        maddpg.reset()\n",
    "        agent_scores = np.zeros(num_agents)\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action     = maddpg.act(state, 1.0)\n",
    "            env_info   = self.env.step(action)[brain_name] \n",
    "            reward     = env_info.rewards\n",
    "            next_state = env_info.vector_observations\n",
    "            done       = env_info.local_done\n",
    "            maddpg.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            state      = next_state\n",
    "            agent_scores += reward\n",
    "            frame_num += 1\n",
    "            \n",
    "            if np.any(done): break\n",
    "                \n",
    "        max_score = np.max(agent_scores)\n",
    "        scores_window.append(max_score)\n",
    "        scores.append(max_score)                \n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_ep, np.mean(scores_window)), end=\"\")\n",
    "        \n",
    "        if i_ep % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_ep, np.mean(scores_window)))\n",
    "\n",
    "        if max_score > current_max:\n",
    "            ddpg_agent.save_checkpoints('checkpoints/checkpoint_max_')\n",
    "            current_max = max_score\n",
    "            \n",
    "        if np.mean(scores_window) >= 0.5:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_ep, np.mean(scores_window)))\n",
    "            break\n",
    "            \n",
    "            \n",
    "    return scores\n",
    "                \n",
    "    \n",
    "    \n",
    "def plot_scores(scores, title=\"\"):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.title(title)\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 0, 'buffer_size': 100000, 'batch_size': 512, 'gamma': 0.99, 'tau': 0.2, 'actor_hidden_sizes': [256, 128], 'lr_actor': 0.0001, 'critic_hidden_sizes': [256, 128], 'lr_critic': 0.0003, 'critic_weight_decay': 0.0, 'mu': 0.0, 'theta': 0.15, 'sigma': 0.2, 'update_every': 5, 'device': device(type='cuda', index=0), 'beta': 0.4, 'beta_decay': 10000}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MADDPG' object has no attribute 'maddpg_agent'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-50f811657470>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mmaddpg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMADDPG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaddpg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mplot_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-0c5701797419>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(maddpg, n_eps, max_steps)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0maction\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mmaddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0menv_info\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mreward\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3dd462b01ae0>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, obs_all_agents, noise)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mobs_all_agents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_all_agents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         actions = [np.clip(agent.act(obs, noise).cpu().data.numpy(), -1, 1) \n\u001b[0;32m---> 34\u001b[0;31m                    for agent, obs in zip(self.maddpg_agent, obs_all_agents)]\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MADDPG' object has no attribute 'maddpg_agent'"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "config.seed = 0\n",
    "config.buffer_size = int(1e5)\n",
    "config.batch_size = 512\n",
    "config.gamma = 0.99\n",
    "config.tau = 2e-1\n",
    "\n",
    "config.actor_hidden_sizes = [256, 128]\n",
    "config.lr_actor = 1e-4\n",
    "config.critic_hidden_sizes = [256, 128]\n",
    "config.lr_critic = 3e-4\n",
    "config.critic_weight_decay = 0.0\n",
    "\n",
    "config.mu = 0.\n",
    "config.theta = 0.15\n",
    "config.sigma = 0.2\n",
    "\n",
    "config.update_every = 5\n",
    "config.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# config.prioritized_replay = True\n",
    "config.beta = 0.4\n",
    "config.beta_decay = 10000\n",
    "# config.alpha = 0.6\n",
    "\n",
    "maddpg = MADDPG(state_size, action_size, config)\n",
    "print(config)\n",
    "scores = train(maddpg, 3000, 2000)\n",
    "\n",
    "plot_scores(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
